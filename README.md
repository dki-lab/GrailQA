# GrailQA: Strongly <ins>G</ins>ene<ins>ra</ins>l<ins>i</ins>zab<ins>l</ins>e Question Answering
<img width="1175" alt="image" src="https://user-images.githubusercontent.com/15921425/110228546-f2193380-7ecf-11eb-8cbd-c5097a064ee4.png">

GrailQA is a new large-scale, high-quality KBQA dataset with 64,331 questions annotated with both answers and corresponding logical forms in different syntax (i.e., SPARQL, S-expression, etc.). It can be used to test three levels of generalization in KBQA: **i.i.d.**, **compositional**, and **zero-shot**.

For dataset and leaderboard, please refer to the [homepage of GrailQA] (https://dki-lab.github.io/GrailQA/). In this repo, we help you to reproduce the results of our baseline models and to train new models using our code.

## Overview
To study the three levels of generalization in KBQA, we implement a line of baseline models of different natures, namely, **Transduction+BERT**, **Transduction+GloVe**, **Ranking+BERT**, and **Ranking+GloVe**. Our implementation is based on AllenNLP 0.9.0. 

## Setup
There are several steps you need to do before running our code.


## Reproduce Our Results
### Reproduce Main Results
### Reproduce Entity Linking Results


## Train New Models
### Training Configuration
### Training Command

## Citation
